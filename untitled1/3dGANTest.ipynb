{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import time\n",
    "import StructureManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Create a Generator Model with hyperparameters values defined as follows\n",
    "    :return: Generator network\n",
    "     \"\"\"\n",
    "    z_size = 200\n",
    "    gen_filters = [64, 32, 16, 1]\n",
    "    gen_kernel_sizes = [2, 4, 4, 4, 4]\n",
    "    gen_strides = [1, 2, 2, 2]\n",
    "    gen_input_shape = (1, 1, 1, z_size)\n",
    "    gen_activations = ['relu', 'relu', 'relu', 'tanh']\n",
    "    gen_convolutional_blocks = 4\n",
    "    \n",
    "    input_layer = layers.Input(shape=gen_input_shape)\n",
    "    \n",
    "    # First 3D transpose convolution(or 3D deconvolution) block\n",
    "    a = layers.Conv3DTranspose(filters=gen_filters[0], \n",
    "                        kernel_size=gen_kernel_sizes[0],\n",
    "                        strides=gen_strides[0])(input_layer)\n",
    "    a = layers.BatchNormalization()(a, training=True)\n",
    "    a = layers.Activation(activation='relu')(a)\n",
    "    \n",
    "    # Next 4 3D transpose convolution(or 3D deconvolution) blocks\n",
    "    for i in range(gen_convolutional_blocks - 1):\n",
    "        a = layers.Conv3DTranspose(filters=gen_filters[i + 1], \n",
    "                            kernel_size=gen_kernel_sizes[i + 1],\n",
    "                            strides=gen_strides[i + 1], padding='same')(a)\n",
    "        a = layers.BatchNormalization()(a, training=True)\n",
    "        a = layers.Activation(activation=gen_activations[i + 1])(a)\n",
    "    gen_model = Model(inputs=input_layer, outputs=a)\n",
    "    gen_model.summary()\n",
    "    return gen_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \"\"\"\n",
    "    Create a Discriminator Model using hyperparameters values defined as follows\n",
    "    :return: Discriminator network\n",
    "    \"\"\"\n",
    "    dis_input_shape = (16, 16, 16, 1)\n",
    "    dis_filters = [16, 32, 64, 1]\n",
    "    dis_kernel_sizes = [4, 4, 4, 4]\n",
    "    dis_strides = [2, 2, 2, 2]\n",
    "    dis_paddings = ['same', 'same', 'same', 'same', 'valid']\n",
    "    dis_alphas = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "    dis_activations = ['leaky_relu', 'leaky_relu', 'leaky_relu', \n",
    "                       'sigmoid']\n",
    "    dis_convolutional_blocks = 4\n",
    "\n",
    "    dis_input_layer = layers.Input(shape=dis_input_shape)\n",
    "    \n",
    "    # The first 3D Convolutional block\n",
    "    a = layers.Conv3D(filters=dis_filters[0],\n",
    "               kernel_size=dis_kernel_sizes[0],\n",
    "               strides=dis_strides[0],\n",
    "               padding=dis_paddings[0])(dis_input_layer)\n",
    "    a = layers.BatchNormalization()(a, training=True)\n",
    "    a = layers.LeakyReLU(dis_alphas[0])(a)\n",
    "    \n",
    "    # Next 4 3D Convolutional Blocks\n",
    "    for i in range(dis_convolutional_blocks - 1):\n",
    "        a = layers.Conv3D(filters=dis_filters[i + 1],\n",
    "                   kernel_size=dis_kernel_sizes[i + 1],\n",
    "                   strides=dis_strides[i + 1],\n",
    "                   padding=dis_paddings[i + 1])(a)\n",
    "        a = layers.BatchNormalization()(a, training=True)\n",
    "        if dis_activations[i + 1] == 'leaky_relu':\n",
    "            a = layers.LeakyReLU(dis_alphas[i + 1])(a)\n",
    "        elif dis_activations[i + 1] == 'sigmoid':\n",
    "            a = layers.Activation(activation='sigmoid')(a)\n",
    "    \n",
    "        dis_model = Model(inputs=dis_input_layer, outputs=a)\n",
    "        print(dis_model.summary())\n",
    "    return dis_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "gen_learning_rate = 0.0006\n",
    "dis_learning_rate = 0.008\n",
    "beta = 0.5\n",
    "batch_size = 4\n",
    "z_size = 200\n",
    "DIR_PATH = ''\n",
    "generated_structures_dir = 'generated_volumes'\n",
    "log_dir = '.\\\\logs'\n",
    "epochs = 100000\n",
    "seed = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model_56\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_29 (InputLayer)        [(None, 1, 1, 1, 200)]    0         \n_________________________________________________________________\nconv3d_transpose_56 (Conv3DT (None, 2, 2, 2, 64)       102464    \n_________________________________________________________________\nbatch_normalization_112 (Bat (None, 2, 2, 2, 64)       256       \n_________________________________________________________________\nactivation_61 (Activation)   (None, 2, 2, 2, 64)       0         \n_________________________________________________________________\nconv3d_transpose_57 (Conv3DT (None, 4, 4, 4, 32)       131104    \n_________________________________________________________________\nbatch_normalization_113 (Bat (None, 4, 4, 4, 32)       128       \n_________________________________________________________________\nactivation_62 (Activation)   (None, 4, 4, 4, 32)       0         \n_________________________________________________________________\nconv3d_transpose_58 (Conv3DT (None, 8, 8, 8, 16)       32784     \n_________________________________________________________________\nbatch_normalization_114 (Bat (None, 8, 8, 8, 16)       64        \n_________________________________________________________________\nactivation_63 (Activation)   (None, 8, 8, 8, 16)       0         \n_________________________________________________________________\nconv3d_transpose_59 (Conv3DT (None, 16, 16, 16, 1)     1025      \n_________________________________________________________________\nbatch_normalization_115 (Bat (None, 16, 16, 16, 1)     4         \n_________________________________________________________________\nactivation_64 (Activation)   (None, 16, 16, 16, 1)     0         \n=================================================================\nTotal params: 267,829\nTrainable params: 267,603\nNon-trainable params: 226\n_________________________________________________________________\n",
      "Model: \"model_57\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_30 (InputLayer)        [(None, 16, 16, 16, 1)]   0         \n_________________________________________________________________\nconv3d_56 (Conv3D)           (None, 8, 8, 8, 16)       1040      \n_________________________________________________________________\nbatch_normalization_116 (Bat (None, 8, 8, 8, 16)       64        \n_________________________________________________________________\nleaky_re_lu_51 (LeakyReLU)   (None, 8, 8, 8, 16)       0         \n_________________________________________________________________\nconv3d_57 (Conv3D)           (None, 4, 4, 4, 32)       32800     \n_________________________________________________________________\nbatch_normalization_117 (Bat (None, 4, 4, 4, 32)       128       \n_________________________________________________________________\nleaky_re_lu_52 (LeakyReLU)   (None, 4, 4, 4, 32)       0         \n=================================================================\nTotal params: 34,032\nTrainable params: 33,936\nNon-trainable params: 96\n_________________________________________________________________\nNone\nModel: \"model_58\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_30 (InputLayer)        [(None, 16, 16, 16, 1)]   0         \n_________________________________________________________________\nconv3d_56 (Conv3D)           (None, 8, 8, 8, 16)       1040      \n_________________________________________________________________\nbatch_normalization_116 (Bat (None, 8, 8, 8, 16)       64        \n_________________________________________________________________\nleaky_re_lu_51 (LeakyReLU)   (None, 8, 8, 8, 16)       0         \n_________________________________________________________________\nconv3d_57 (Conv3D)           (None, 4, 4, 4, 32)       32800     \n_________________________________________________________________\nbatch_normalization_117 (Bat (None, 4, 4, 4, 32)       128       \n_________________________________________________________________\nleaky_re_lu_52 (LeakyReLU)   (None, 4, 4, 4, 32)       0         \n_________________________________________________________________\nconv3d_58 (Conv3D)           (None, 2, 2, 2, 64)       131136    \n_________________________________________________________________\nbatch_normalization_118 (Bat (None, 2, 2, 2, 64)       256       \n_________________________________________________________________\nleaky_re_lu_53 (LeakyReLU)   (None, 2, 2, 2, 64)       0         \n=================================================================\nTotal params: 165,424\nTrainable params: 165,200\nNon-trainable params: 224\n_________________________________________________________________\nNone\nModel: \"model_59\"",
      "\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_30 (InputLayer)        [(None, 16, 16, 16, 1)]   0         \n_________________________________________________________________\nconv3d_56 (Conv3D)           (None, 8, 8, 8, 16)       1040      \n_________________________________________________________________\nbatch_normalization_116 (Bat (None, 8, 8, 8, 16)       64        \n_________________________________________________________________\nleaky_re_lu_51 (LeakyReLU)   (None, 8, 8, 8, 16)       0         \n_________________________________________________________________\nconv3d_57 (Conv3D)           (None, 4, 4, 4, 32)       32800     \n_________________________________________________________________\nbatch_normalization_117 (Bat (None, 4, 4, 4, 32)       128       \n_________________________________________________________________\nleaky_re_lu_52 (LeakyReLU)   (None, 4, 4, 4, 32)       0         \n_________________________________________________________________\nconv3d_58 (Conv3D)           (None, 2, 2, 2, 64)       131136    \n_________________________________________________________________\nbatch_normalization_118 (Bat (None, 2, 2, 2, 64)       256       \n_________________________________________________________________\nleaky_re_lu_53 (LeakyReLU)   (None, 2, 2, 2, 64)       0         \n_________________________________________________________________\nconv3d_59 (Conv3D)           (None, 1, 1, 1, 1)        4097      \n_________________________________________________________________\nbatch_normalization_119 (Bat (None, 1, 1, 1, 1)        4         \n_________________________________________________________________\nactivation_65 (Activation)   (None, 1, 1, 1, 1)        0         \n=================================================================\nTotal params: 169,525\nTrainable params: 169,299\nNon-trainable params: 226\n_________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Create instances\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Specify optimizer \n",
    "gen_optimizer = tf.keras.optimizers.Adam(lr=gen_learning_rate, beta_1=beta)\n",
    "dis_optimizer = tf.keras.optimizers.Adam(lr=dis_learning_rate, beta_1=0.9)\n",
    "\n",
    "# Compile networks\n",
    "generator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=dis_optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "adversarial_model = tf.keras.models.Sequential()\n",
    "adversarial_model.add(generator)\n",
    "adversarial_model.add(discriminator)\n",
    "adversarial_model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=gen_learning_rate, beta_1=beta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "dir = \"{}\\{}\".format(log_dir, time.time())\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=dir)\n",
    "tensorboard.set_model(generator)\n",
    "tensorboard.set_model(discriminator)\n",
    "writer = tf.summary.create_file_writer(dir)\n",
    "writer.set_as_default()\n",
    "labels_real = np.reshape([1] * batch_size, (-1, 1, 1, 1, 1))\n",
    "labels_fake = np.reshape([0] * batch_size, (-1, 1, 1, 1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "d = \"C:\\\\Users\\\\Simba\\\\Desktop\\\\Minecraft Server\\\\databuilds\\\\lamps\\\\\"\n",
    "d2 = \"C:\\\\Users\\\\Simba\\\\Desktop\\\\Minecraft Server\\\\databuilds\\\\generated\\\\minecraft\\\\structures\"\n",
    "dataset_list = StructureManager.load_structure_blocks(d2, (16,16,16))\n",
    "\n",
    "scalar = len(StructureManager.globalPalette)-1\n",
    "processed_inputs = np.subtract(np.multiply(np.divide(dataset_list, scalar),2),1)\n",
    "#processed_inputs = processed_inputs.reshape((processed_inputs.shape[0], processed_inputs.shape[1], processed_inputs.shape[3], processed_inputs.shape[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 16, 16, 16)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 124
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def generate_save_structures(model, epoch, test_input):\n",
    "  predictions = model(test_input, training=False)\n",
    "  processed_predictions = np.divide(np.multiply(np.add(predictions, 1), scalar),2)\n",
    "  processed_predictions = processed_predictions.astype(int)\n",
    "  for prediction in processed_predictions:\n",
    "      StructureManager.create_nbt_from_3d(np.squeeze(prediction), epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def write_log(name, value, epoch):\n",
    "    tf.summary.scalar(name, value, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Epoch: 200\n",
      "Epoch: 201\n",
      "Epoch: 202\n",
      "Epoch: 203\n",
      "Epoch: 204\n",
      "Epoch: 205\n",
      "Epoch: 206\n",
      "Epoch: 207\n",
      "Epoch: 208\n",
      "Epoch: 209\n",
      "Epoch: 210\n",
      "Epoch: 211\n",
      "Epoch: 212\n",
      "Epoch: 213\n",
      "Epoch: 214\n",
      "Epoch: 215\n",
      "Epoch: 216\n",
      "Epoch: 217\n",
      "Epoch: 218\n",
      "Epoch: 219\n",
      "Epoch: 220\n",
      "Epoch: 221\n",
      "Epoch: 222\n",
      "Epoch: 223\n",
      "Epoch: 224\n",
      "Epoch: 225\n",
      "Epoch: 226\n",
      "Epoch: 227\n",
      "Epoch: 228\n",
      "Epoch: 229\n",
      "Epoch: 230\n",
      "Epoch: 231\n",
      "Epoch: 232\n",
      "Epoch: 233\n",
      "Epoch: 234\n",
      "Epoch: 235\n",
      "Epoch: 236\n",
      "Epoch: 237\n",
      "Epoch: 238\n",
      "Epoch: 239\n",
      "Epoch: 240\n",
      "Epoch: 241\n",
      "Epoch: 242\n",
      "Epoch: 243\n",
      "Epoch: 244\n",
      "Epoch: 245\n",
      "Epoch: 246\n",
      "Epoch: 247\n",
      "Epoch: 248\n",
      "Epoch: 249\n",
      "Epoch: 250\n",
      "Epoch: 251\n",
      "Epoch: 252\n",
      "Epoch: 253\n",
      "Epoch: 254\n",
      "Epoch: 255\n",
      "Epoch: 256\n",
      "Epoch: 257\n",
      "Epoch: 258\n",
      "Epoch: 259\n",
      "Epoch: 260\n",
      "Epoch: 261\n",
      "Epoch: 262\n",
      "Epoch: 263\n",
      "Epoch: 264\n",
      "Epoch: 265\n",
      "Epoch: 266\n",
      "Epoch: 267\n",
      "Epoch: 268\n",
      "Epoch: 269\n",
      "Epoch: 270\n",
      "Epoch: 271\n",
      "Epoch: 272\n",
      "Epoch: 273\n",
      "Epoch: 274\n",
      "Epoch: 275\n",
      "Epoch: 276\n",
      "Epoch: 277\n",
      "Epoch: 278\n",
      "Epoch: 279\n",
      "Epoch: 280\n",
      "Epoch: 281\n",
      "Epoch: 282\n",
      "Epoch: 283\n",
      "Epoch: 284\n",
      "Epoch: 285\n",
      "Epoch: 286\n",
      "Epoch: 287\n",
      "Epoch: 288\n",
      "Epoch: 289\n",
      "Epoch: 290\n",
      "Epoch: 291\n",
      "Epoch: 292\n",
      "Epoch: 293\n",
      "Epoch: 294\n",
      "Epoch: 295\n",
      "Epoch: 296\n",
      "Epoch: 297\n",
      "Epoch: 298\n",
      "Epoch: 299\n",
      "Epoch: 300\n",
      "Epoch: 301\n",
      "Epoch: 302\n",
      "Epoch: 303\n",
      "Epoch: 304\n",
      "Epoch: 305\n",
      "Epoch: 306\n",
      "Epoch: 307\n",
      "Epoch: 308\n",
      "Epoch: 309\n",
      "Epoch: 310\n",
      "Epoch: 311\n",
      "Epoch: 312\n",
      "Epoch: 313\n",
      "Epoch: 314\n",
      "Epoch: 315\n",
      "Epoch: 316\n",
      "Epoch: 317\n",
      "Epoch: 318\n",
      "Epoch: 319\n",
      "Epoch: 320\n",
      "Epoch: 321\n",
      "Epoch: 322\n",
      "Epoch: 323\n",
      "Epoch: 324\n",
      "Epoch: 325\n",
      "Epoch: 326\n",
      "Epoch: 327\n",
      "Epoch: 328\n",
      "Epoch: 329\n",
      "Epoch: 330\n",
      "Epoch: 331\n",
      "Epoch: 332\n",
      "Epoch: 333\n",
      "Epoch: 334\n",
      "Epoch: 335\n",
      "Epoch: 336\n",
      "Epoch: 337\n",
      "Epoch: 338\n",
      "Epoch: 339\n",
      "Epoch: 340\n",
      "Epoch: 341\n",
      "Epoch: 342\n",
      "Epoch: 343\n",
      "Epoch: 344\n",
      "Epoch: 345\n",
      "Epoch: 346\n",
      "Epoch: 347\n",
      "Epoch: 348\n",
      "Epoch: 349\n",
      "Epoch: 350\n",
      "Epoch: 351\n",
      "Epoch: 352\n",
      "Epoch: 353\n",
      "Epoch: 354\n",
      "Epoch: 355\n",
      "Epoch: 356\n",
      "Epoch: 357\n",
      "Epoch: 358\n",
      "Epoch: 359\n",
      "Epoch: 360\n",
      "Epoch: 361\n",
      "Epoch: 362\n",
      "Epoch: 363\n",
      "Epoch: 364\n",
      "Epoch: 365\n",
      "Epoch: 366\n",
      "Epoch: 367\n",
      "Epoch: 368\n",
      "Epoch: 369\n",
      "Epoch: 370\n",
      "Epoch: 371\n",
      "Epoch: 372\n",
      "Epoch: 373\n",
      "Epoch: 374\n",
      "Epoch: 375\n",
      "Epoch: 376\n",
      "Epoch: 377\n",
      "Epoch: 378\n",
      "Epoch: 379\n",
      "Epoch: 380\n",
      "Epoch: 381\n",
      "Epoch: 382\n",
      "Epoch: 383\n",
      "Epoch: 384\n",
      "Epoch: 385\n",
      "Epoch: 386\n",
      "Epoch: 387\n",
      "Epoch: 388\n",
      "Epoch: 389\n",
      "Epoch: 390\n",
      "Epoch: 391\n",
      "Epoch: 392\n",
      "Epoch: 393\n",
      "Epoch: 394\n",
      "Epoch: 395\n",
      "Epoch: 396\n",
      "Epoch: 397\n",
      "Epoch: 398\n",
      "Epoch: 399\n",
      "Epoch: 400\n",
      "Epoch: 401\n",
      "Epoch: 402\n",
      "Epoch: 403\n",
      "Epoch: 404\n",
      "Epoch: 405\n",
      "Epoch: 406\n",
      "Epoch: 407\n",
      "Epoch: 408\n",
      "Epoch: 409\n",
      "Epoch: 410\n",
      "Epoch: 411\n",
      "Epoch: 412\n",
      "Epoch: 413\n",
      "Epoch: 414\n",
      "Epoch: 415\n",
      "Epoch: 416\n",
      "Epoch: 417\n",
      "Epoch: 418\n",
      "Epoch: 419\n",
      "Epoch: 420\n",
      "Epoch: 421\n",
      "Epoch: 422\n",
      "Epoch: 423\n",
      "Epoch: 424\n",
      "Epoch: 425\n",
      "Epoch: 426\n",
      "Epoch: 427\n",
      "Epoch: 428\n",
      "Epoch: 429\n",
      "Epoch: 430\n",
      "Epoch: 431\n",
      "Epoch: 432\n",
      "Epoch: 433\n",
      "Epoch: 434\n",
      "Epoch: 435\n",
      "Epoch: 436\n",
      "Epoch: 437\n",
      "Epoch: 438\n",
      "Epoch: 439\n",
      "Epoch: 440\n",
      "Epoch: 441\n",
      "Epoch: 442\n",
      "Epoch: 443\n",
      "Epoch: 444\n",
      "Epoch: 445\n",
      "Epoch: 446\n",
      "Epoch: 447\n",
      "Epoch: 448\n",
      "Epoch: 449\n",
      "Epoch: 450\n",
      "Epoch: 451\n",
      "Epoch: 452\n",
      "Epoch: 453\n",
      "Epoch: 454\n",
      "Epoch: 455\n",
      "Epoch: 456\n",
      "Epoch: 457\n",
      "Epoch: 458\n",
      "Epoch: 459\n",
      "Epoch: 460\n",
      "Epoch: 461\n",
      "Epoch: 462\n",
      "Epoch: 463\n",
      "Epoch: 464\n",
      "Epoch: 465\n",
      "Epoch: 466\n",
      "Epoch: 467\n",
      "Epoch: 468\n",
      "Epoch: 469\n",
      "Epoch: 470\n",
      "Epoch: 471\n",
      "Epoch: 472\n",
      "Epoch: 473\n",
      "Epoch: 474\n",
      "Epoch: 475\n",
      "Epoch: 476\n",
      "Epoch: 477\n",
      "Epoch: 478\n",
      "Epoch: 479\n",
      "Epoch: 480\n",
      "Epoch: 481\n",
      "Epoch: 482\n",
      "Epoch: 483\n",
      "Epoch: 484\n",
      "Epoch: 485\n",
      "Epoch: 486\n",
      "Epoch: 487\n",
      "Epoch: 488\n",
      "Epoch: 489\n",
      "Epoch: 490\n",
      "Epoch: 491\n",
      "Epoch: 492\n",
      "Epoch: 493\n",
      "Epoch: 494\n",
      "Epoch: 495\n",
      "Epoch: 496\n",
      "Epoch: 497\n",
      "Epoch: 498\n",
      "Epoch: 499\n",
      "Epoch: 500\n",
      "Epoch: 501\n",
      "Epoch: 502\n",
      "Epoch: 503\n",
      "Epoch: 504\n",
      "Epoch: 505\n",
      "Epoch: 506\n",
      "Epoch: 507\n",
      "Epoch: 508\n",
      "Epoch: 509\n",
      "Epoch: 510\n",
      "Epoch: 511\n",
      "Epoch: 512\n",
      "Epoch: 513\n",
      "Epoch: 514\n",
      "Epoch: 515\n",
      "Epoch: 516\n",
      "Epoch: 517\n",
      "Epoch: 518\n",
      "Epoch: 519\n",
      "Epoch: 520\n",
      "Epoch: 521\n",
      "Epoch: 522\n",
      "Epoch: 523\n",
      "Epoch: 524\n",
      "Epoch: 525\n",
      "Epoch: 526\n",
      "Epoch: 527\n",
      "Epoch: 528\n",
      "Epoch: 529\n",
      "Epoch: 530\n",
      "Epoch: 531\n",
      "Epoch: 532\n",
      "Epoch: 533\n",
      "Epoch: 534\n",
      "Epoch: 535\n",
      "Epoch: 536\n",
      "Epoch: 537\n",
      "Epoch: 538\n",
      "Epoch: 539\n",
      "Epoch: 540\n",
      "Epoch: 541\n",
      "Epoch: 542\n",
      "Epoch: 543\n",
      "Epoch: 544\n",
      "Epoch: 545\n",
      "Epoch: 546\n",
      "Epoch: 547\n",
      "Epoch: 548\n",
      "Epoch: 549\n",
      "Epoch: 550\n",
      "Epoch: 551\n",
      "Epoch: 552\n",
      "Epoch: 553\n",
      "Epoch: 554\n",
      "Epoch: 555\n",
      "Epoch: 556\n",
      "Epoch: 557\n",
      "Epoch: 558\n",
      "Epoch: 559\n",
      "Epoch: 560\n",
      "Epoch: 561\n",
      "Epoch: 562\n",
      "Epoch: 563\n",
      "Epoch: 564\n",
      "Epoch: 565\n",
      "Epoch: 566\n",
      "Epoch: 567\n",
      "Epoch: 568\n",
      "Epoch: 569\n",
      "Epoch: 570\n",
      "Epoch: 571\n",
      "Epoch: 572\n",
      "Epoch: 573\n",
      "Epoch: 574\n",
      "Epoch: 575\n",
      "Epoch: 576\n",
      "Epoch: 577\n",
      "Epoch: 578\n",
      "Epoch: 579\n",
      "Epoch: 580\n",
      "Epoch: 581\n",
      "Epoch: 582\n",
      "Epoch: 583\n",
      "Epoch: 584\n",
      "Epoch: 585\n",
      "Epoch: 586\n",
      "Epoch: 587\n",
      "Epoch: 588\n",
      "Epoch: 589\n",
      "Epoch: 590\n",
      "Epoch: 591\n",
      "Epoch: 592\n",
      "Epoch: 593\n",
      "Epoch: 594\n",
      "Epoch: 595\n",
      "Epoch: 596\n",
      "Epoch: 597\n",
      "Epoch: 598\n",
      "Epoch: 599\n",
      "Epoch: 600\n",
      "Epoch: 601\n",
      "Epoch: 602\n",
      "Epoch: 603\n",
      "Epoch: 604\n",
      "Epoch: 605\n",
      "Epoch: 606\n",
      "Epoch: 607\n",
      "Epoch: 608\n",
      "Epoch: 609\n",
      "Epoch: 610\n",
      "Epoch: 611\n",
      "Epoch: 612\n",
      "Epoch: 613\n",
      "Epoch: 614\n",
      "Epoch: 615\n",
      "Epoch: 616\n",
      "Epoch: 617\n",
      "Epoch: 618\n",
      "Epoch: 619\n",
      "Epoch: 620\n",
      "Epoch: 621\n",
      "Epoch: 622\n",
      "Epoch: 623\n",
      "Epoch: 624\n",
      "Epoch: 625\n",
      "Epoch: 626\n",
      "Epoch: 627\n",
      "Epoch: 628\n",
      "Epoch: 629\n",
      "Epoch: 630\n",
      "Epoch: 631\n",
      "Epoch: 632\n",
      "Epoch: 633\n",
      "Epoch: 634\n",
      "Epoch: 635\n",
      "Epoch: 636\n",
      "Epoch: 637\n",
      "Epoch: 638\n",
      "Epoch: 639\n",
      "Epoch: 640\n",
      "Epoch: 641\n",
      "Epoch: 642\n",
      "Epoch: 643\n",
      "Epoch: 644\n",
      "Epoch: 645\n",
      "Epoch: 646\n",
      "Epoch: 647\n",
      "Epoch: 648\n",
      "Epoch: 649\n",
      "Epoch: 650\n",
      "Epoch: 651\n",
      "Epoch: 652\n",
      "Epoch: 653\n",
      "Epoch: 654\n",
      "Epoch: 655\n",
      "Epoch: 656\n",
      "Epoch: 657\n",
      "Epoch: 658\n",
      "Epoch: 659\n",
      "Epoch: 660\n",
      "Epoch: 661\n",
      "Epoch: 662\n",
      "Epoch: 663\n",
      "Epoch: 664\n",
      "Epoch: 665\n",
      "Epoch: 666\n",
      "Epoch: 667\n",
      "Epoch: 668\n",
      "Epoch: 669\n",
      "Epoch: 670\n",
      "Epoch: 671\n",
      "Epoch: 672\n",
      "Epoch: 673\n",
      "Epoch: 674\n",
      "Epoch: 675\n",
      "Epoch: 676\n",
      "Epoch: 677\n",
      "Epoch: 678\n",
      "Epoch: 679\n",
      "Epoch: 680\n",
      "Epoch: 681\n",
      "Epoch: 682\n",
      "Epoch: 683\n",
      "Epoch: 684\n",
      "Epoch: 685\n",
      "Epoch: 686\n",
      "Epoch: 687\n",
      "Epoch: 688\n",
      "Epoch: 689\n",
      "Epoch: 690\n",
      "Epoch: 691\n",
      "Epoch: 692\n",
      "Epoch: 693\n",
      "Epoch: 694\n",
      "Epoch: 695\n",
      "Epoch: 696\n",
      "Epoch: 697\n",
      "Epoch: 698\n",
      "Epoch: 699\n",
      "Epoch: 700\n",
      "Epoch: 701\n",
      "Epoch: 702\n",
      "Epoch: 703\n",
      "Epoch: 704\n",
      "Epoch: 705\n",
      "Epoch: 706\n",
      "Epoch: 707\n",
      "Epoch: 708\n",
      "Epoch: 709\n",
      "Epoch: 710\n",
      "Epoch: 711\n",
      "Epoch: 712\n",
      "Epoch: 713\n",
      "Epoch: 714\n",
      "Epoch: 715\n",
      "Epoch: 716\n",
      "Epoch: 717\n",
      "Epoch: 718\n",
      "Epoch: 719\n",
      "Epoch: 720\n",
      "Epoch: 721\n",
      "Epoch: 722\n",
      "Epoch: 723\n",
      "Epoch: 724\n",
      "Epoch: 725\n",
      "Epoch: 726\n",
      "Epoch: 727\n",
      "Epoch: 728\n",
      "Epoch: 729\n",
      "Epoch: 730\n",
      "Epoch: 731\n",
      "Epoch: 732\n",
      "Epoch: 733\n",
      "Epoch: 734\n",
      "Epoch: 735\n",
      "Epoch: 736\n",
      "Epoch: 737\n",
      "Epoch: 738\n",
      "Epoch: 739\n",
      "Epoch: 740\n",
      "Epoch: 741\n",
      "Epoch: 742\n",
      "Epoch: 743\n",
      "Epoch: 744\n",
      "Epoch: 745\n",
      "Epoch: 746\n",
      "Epoch: 747\n",
      "Epoch: 748\n",
      "Epoch: 749\n",
      "Epoch: 750\n",
      "Epoch: 751\n",
      "Epoch: 752\n",
      "Epoch: 753\n",
      "Epoch: 754\n",
      "Epoch: 755\n",
      "Epoch: 756\n",
      "Epoch: 757\n",
      "Epoch: 758\n",
      "Epoch: 759\n",
      "Epoch: 760\n",
      "Epoch: 761\n",
      "Epoch: 762\n",
      "Epoch: 763\n",
      "Epoch: 764\n",
      "Epoch: 765\n",
      "Epoch: 766\n",
      "Epoch: 767\n",
      "Epoch: 768\n",
      "Epoch: 769\n",
      "Epoch: 770\n",
      "Epoch: 771\n",
      "Epoch: 772\n",
      "Epoch: 773\n",
      "Epoch: 774\n",
      "Epoch: 775\n",
      "Epoch: 776\n",
      "Epoch: 777\n",
      "Epoch: 778\n",
      "Epoch: 779\n",
      "Epoch: 780\n",
      "Epoch: 781\n",
      "Epoch: 782\n",
      "Epoch: 783\n",
      "Epoch: 784\n",
      "Epoch: 785\n",
      "Epoch: 786\n",
      "Epoch: 787\n",
      "Epoch: 788\n",
      "Epoch: 789\n",
      "Epoch: 790\n",
      "Epoch: 791\n",
      "Epoch: 792\n",
      "Epoch: 793\n",
      "Epoch: 794\n",
      "Epoch: 795\n",
      "Epoch: 796\n",
      "Epoch: 797\n",
      "Epoch: 798\n",
      "Epoch: 799\n",
      "Epoch: 800\n",
      "Epoch: 801\n",
      "Epoch: 802\n",
      "Epoch: 803\n",
      "Epoch: 804\n",
      "Epoch: 805\n",
      "Epoch: 806\n",
      "Epoch: 807\n",
      "Epoch: 808\n",
      "Epoch: 809\n",
      "Epoch: 810\n",
      "Epoch: 811\n",
      "Epoch: 812\n",
      "Epoch: 813\n",
      "Epoch: 814\n",
      "Epoch: 815\n",
      "Epoch: 816\n",
      "Epoch: 817\n",
      "Epoch: 818\n",
      "Epoch: 819\n",
      "Epoch: 820\n",
      "Epoch: 821\n",
      "Epoch: 822\n",
      "Epoch: 823\n",
      "Epoch: 824\n",
      "Epoch: 825\n",
      "Epoch: 826\n",
      "Epoch: 827\n",
      "Epoch: 828\n",
      "Epoch: 829\n",
      "Epoch: 830\n",
      "Epoch: 831\n",
      "Epoch: 832\n",
      "Epoch: 833\n",
      "Epoch: 834\n",
      "Epoch: 835\n",
      "Epoch: 836\n",
      "Epoch: 837\n",
      "Epoch: 838\n",
      "Epoch: 839\n",
      "Epoch: 840\n",
      "Epoch: 841\n",
      "Epoch: 842\n",
      "Epoch: 843\n",
      "Epoch: 844\n",
      "Epoch: 845\n",
      "Epoch: 846\n",
      "Epoch: 847\n",
      "Epoch: 848\n",
      "Epoch: 849\n",
      "Epoch: 850\n",
      "Epoch: 851\n",
      "Epoch: 852\n",
      "Epoch: 853\n",
      "Epoch: 854\n",
      "Epoch: 855\n",
      "Epoch: 856\n",
      "Epoch: 857\n",
      "Epoch: 858\n",
      "Epoch: 859\n",
      "Epoch: 860\n",
      "Epoch: 861\n",
      "Epoch: 862\n",
      "Epoch: 863\n",
      "Epoch: 864\n",
      "Epoch: 865\n",
      "Epoch: 866\n",
      "Epoch: 867\n",
      "Epoch: 868\n",
      "Epoch: 869\n",
      "Epoch: 870\n",
      "Epoch: 871\n",
      "Epoch: 872\n",
      "Epoch: 873\n",
      "Epoch: 874\n",
      "Epoch: 875\n",
      "Epoch: 876\n",
      "Epoch: 877\n",
      "Epoch: 878\n",
      "Epoch: 879\n",
      "Epoch: 880\n",
      "Epoch: 881\n",
      "Epoch: 882\n",
      "Epoch: 883\n",
      "Epoch: 884\n",
      "Epoch: 885\n",
      "Epoch: 886\n",
      "Epoch: 887\n",
      "Epoch: 888\n",
      "Epoch: 889\n",
      "Epoch: 890\n",
      "Epoch: 891\n",
      "Epoch: 892\n",
      "Epoch: 893\n",
      "Epoch: 894\n",
      "Epoch: 895\n",
      "Epoch: 896\n",
      "Epoch: 897\n",
      "Epoch: 898\n",
      "Epoch: 899\n",
      "Epoch: 900\n",
      "Epoch: 901\n",
      "Epoch: 902\n",
      "Epoch: 903\n",
      "Epoch: 904\n",
      "Epoch: 905\n",
      "Epoch: 906\n",
      "Epoch: 907\n",
      "Epoch: 908\n",
      "Epoch: 909\n",
      "Epoch: 910\n",
      "Epoch: 911\n",
      "Epoch: 912\n",
      "Epoch: 913\n",
      "Epoch: 914\n",
      "Epoch: 915\n",
      "Epoch: 916\n",
      "Epoch: 917\n",
      "Epoch: 918\n",
      "Epoch: 919\n",
      "Epoch: 920\n",
      "Epoch: 921\n",
      "Epoch: 922\n",
      "Epoch: 923\n",
      "Epoch: 924\n",
      "Epoch: 925\n",
      "Epoch: 926\n",
      "Epoch: 927\n",
      "Epoch: 928\n",
      "Epoch: 929\n",
      "Epoch: 930\n",
      "Epoch: 931\n",
      "Epoch: 932\n",
      "Epoch: 933\n",
      "Epoch: 934\n",
      "Epoch: 935\n",
      "Epoch: 936\n",
      "Epoch: 937\n",
      "Epoch: 938\n",
      "Epoch: 939\n",
      "Epoch: 940\n",
      "Epoch: 941\n",
      "Epoch: 942\n",
      "Epoch: 943\n",
      "Epoch: 944\n",
      "Epoch: 945\n",
      "Epoch: 946\n",
      "Epoch: 947\n",
      "Epoch: 948\n",
      "Epoch: 949\n",
      "Epoch: 950\n",
      "Epoch: 951\n",
      "Epoch: 952\n",
      "Epoch: 953\n",
      "Epoch: 954\n",
      "Epoch: 955\n",
      "Epoch: 956\n",
      "Epoch: 957\n",
      "Epoch: 958\n",
      "Epoch: 959\n",
      "Epoch: 960\n",
      "Epoch: 961\n",
      "Epoch: 962\n",
      "Epoch: 963\n",
      "Epoch: 964\n",
      "Epoch: 965\n",
      "Epoch: 966\n",
      "Epoch: 967\n",
      "Epoch: 968\n",
      "Epoch: 969\n",
      "Epoch: 970\n",
      "Epoch: 971\n",
      "Epoch: 972\n",
      "Epoch: 973\n",
      "Epoch: 974\n",
      "Epoch: 975\n",
      "Epoch: 976\n",
      "Epoch: 977\n",
      "Epoch: 978\n",
      "Epoch: 979\n",
      "Epoch: 980\n",
      "Epoch: 981\n",
      "Epoch: 982\n",
      "Epoch: 983\n",
      "Epoch: 984\n",
      "Epoch: 985\n",
      "Epoch: 986\n",
      "Epoch: 987\n",
      "Epoch: 988\n",
      "Epoch: 989\n",
      "Epoch: 990\n",
      "Epoch: 991\n",
      "Epoch: 992\n",
      "Epoch: 993\n",
      "Epoch: 994\n",
      "Epoch: 995\n",
      "Epoch: 996\n",
      "Epoch: 997\n",
      "Epoch: 998\n",
      "Epoch: 999\n",
      "Epoch: 1000\n",
      "Epoch: 1001\n",
      "Epoch: 1002\n",
      "Epoch: 1003\n",
      "Epoch: 1004\n",
      "Epoch: 1005\n",
      "Epoch: 1006\n",
      "Epoch: 1007\n",
      "Epoch: 1008\n",
      "Epoch: 1009\n",
      "Epoch: 1010\n",
      "Epoch: 1011\n",
      "Epoch: 1012\n",
      "Epoch: 1013\n",
      "Epoch: 1014\n",
      "Epoch: 1015\n",
      "Epoch: 1016\n",
      "Epoch: 1017\n",
      "Epoch: 1018\n",
      "Epoch: 1019\n",
      "Epoch: 1020\n",
      "Epoch: 1021\n",
      "Epoch: 1022\n",
      "Epoch: 1023\n",
      "Epoch: 1024\n",
      "Epoch: 1025\n",
      "Epoch: 1026\n",
      "Epoch: 1027\n",
      "Epoch: 1028\n",
      "Epoch: 1029\n",
      "Epoch: 1030\n",
      "Epoch: 1031\n",
      "Epoch: 1032\n",
      "Epoch: 1033\n",
      "Epoch: 1034\n",
      "Epoch: 1035\n",
      "Epoch: 1036\n",
      "Epoch: 1037\n",
      "Epoch: 1038\n",
      "Epoch: 1039\n",
      "Epoch: 1040\n",
      "Epoch: 1041\n",
      "Epoch: 1042\n",
      "Epoch: 1043\n",
      "Epoch: 1044\n",
      "Epoch: 1045\n",
      "Epoch: 1046\n",
      "Epoch: 1047\n",
      "Epoch: 1048\n",
      "Epoch: 1049\n",
      "Epoch: 1050\n",
      "Epoch: 1051\n",
      "Epoch: 1052\n",
      "Epoch: 1053\n",
      "Epoch: 1054\n",
      "Epoch: 1055\n",
      "Epoch: 1056\n",
      "Epoch: 1057\n",
      "Epoch: 1058\n",
      "Epoch: 1059\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "counter = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "    number_of_batches = int(processed_inputs.shape[0] / batch_size)\n",
    "    for index in range(number_of_batches):\n",
    "        z_sample = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, \n",
    "                            z_size]).astype(np.float32)\n",
    "        struct_batch = processed_inputs[index * batch_size:(index + 1) * batch_size, :, :, :]\n",
    "        gen_structs = generator.predict(z_sample)\n",
    "        # Make the discriminator network trainable\n",
    "        discriminator.trainable = True\n",
    "\n",
    "        # Train the discriminator network\n",
    "        struct_batch = struct_batch.reshape((struct_batch.shape[0],\n",
    "                                             struct_batch.shape[1],\n",
    "                                             struct_batch.shape[2],\n",
    "                                             struct_batch.shape[3],\n",
    "                                             1))\n",
    "        if index % 1 == 0:\n",
    "            loss_real = discriminator.train_on_batch(struct_batch, \n",
    "                                                 labels_real)\n",
    "            loss_fake = discriminator.train_on_batch(gen_structs, \n",
    "                                                 labels_fake)        \n",
    "            d_loss = 0.5 * np.add(loss_real, loss_fake)\n",
    "        else:\n",
    "            d_loss = 0\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        z = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
    "        # Train the adversarial model\n",
    "        g_loss = adversarial_model.train_on_batch(z, labels_real)\n",
    "        \n",
    "        gen_losses.append(g_loss)\n",
    "        dis_losses.append(d_loss)\n",
    "\n",
    "        write_log('g_loss', np.mean(gen_losses), counter)\n",
    "        write_log('d_loss', np.mean(dis_losses), counter)\n",
    "        counter+=1\n",
    "    if epoch % 10 == 0:\n",
    "        generate_save_structures(generator, epoch, seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for ind, block in enumerate(StructureManager.globalPalette):\n",
    "    print(ind)\n",
    "    print(block['Name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 60
    }
   ],
   "source": [
    "\n",
    "discriminator.train_on_batch(gen_structs,labels_fake)  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 16, 16, 16, 1)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 61
    }
   ],
   "source": [
    "gen_structs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}